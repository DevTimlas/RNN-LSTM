{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import GRU, LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>twitts</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>does any1 have the spousal abuse number? I jus...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2117</th>\n",
       "      <td>@BigSithewineguy Hey there. Looks like, on BBR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>is heartbroken</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>I dont wanna leave San Diego   but I pack anyw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>Tonight at Mission Street Food: Trumpet mushro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 twitts  sentiment\n",
       "93    does any1 have the spousal abuse number? I jus...          0\n",
       "2117  @BigSithewineguy Hey there. Looks like, on BBR...          1\n",
       "839                                     is heartbroken           0\n",
       "841   I dont wanna leave San Diego   but I pack anyw...          0\n",
       "609   Tonight at Mission Street Food: Trumpet mushro...          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/home/tim/Datasets/twitter4000.csv')\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "twitts       Unsuccessfully tried to get my friend's car ke...\n",
       "sentiment                                                    1\n",
       "Name: 3333, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[3333]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 µs, sys: 2 µs, total: 13 µs\n",
      "Wall time: 25 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "def clean_text(txt):\n",
    "    contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \n",
    "                        \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\", \n",
    "                        \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\",\n",
    "                        \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \n",
    "                        \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
    "                        \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\n",
    "                        \"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
    "                        \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\",\n",
    "                        \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
    "                        \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\",\n",
    "                        \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                        \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \n",
    "                        \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                        \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n",
    "                        \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                        \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n",
    "                        \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n",
    "                        \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "                        \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \n",
    "                        \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                        \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
    "                        \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
    "                        \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
    "                        \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\",\n",
    "                        \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
    "                        \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n",
    "                        \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
    "                        \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                        \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
    "                        \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\",\n",
    "                        \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                        \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
    "                        \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n",
    "                        \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n",
    "                        \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                        \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "        \n",
    "    def _get_contractions(contraction_dict):\n",
    "        contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
    "        return contraction_dict, contraction_re\n",
    "    \n",
    "    def replace_contractions(text):\n",
    "        contractions, contractions_re = _get_contractions(contraction_dict)\n",
    "        def replace(match):\n",
    "            return contractions[match.group(0)]\n",
    "        return contractions_re.sub(replace, text)\n",
    "    \n",
    "    txt = replace_contractions(txt)\n",
    "    \n",
    "    txt = \"\".join([char for char in txt if char not in string.punctuation])\n",
    "    txt = re.sub('[0-9]+', '', txt)\n",
    "    \n",
    "    words = word_tokenize(txt)\n",
    "    \n",
    "#    stop_words = set(stopwords.words('english'))\n",
    "#    words = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    \n",
    "    cleaned_text = ' '.join(words)\n",
    "    return cleaned_text\n",
    "\n",
    "data['cleaned_text'] = data['twitts'].apply(lambda txt: clean_text(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>twitts</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>arghhh pissed off that essendon lost</td>\n",
       "      <td>0</td>\n",
       "      <td>arghhh pissed off that essendon lost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2876</th>\n",
       "      <td>@peterfacinelli ..watching son playing soccer....</td>\n",
       "      <td>1</td>\n",
       "      <td>peterfacinelli watching son playing soccerin t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>why doesn't my pic show up when I tweetsearch?</td>\n",
       "      <td>0</td>\n",
       "      <td>why does not my pic show up when I tweetsearch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988</th>\n",
       "      <td>@TiaMowry  CW sux for dropping the show. Thank...</td>\n",
       "      <td>1</td>\n",
       "      <td>TiaMowry CW sux for dropping the show Thanks G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>@marcusjroberts working on a Sunday sucks  don...</td>\n",
       "      <td>0</td>\n",
       "      <td>marcusjroberts working on a Sunday sucks do no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1203</th>\n",
       "      <td>It's hot.  Kinda wishing I did this. Bummer. G...</td>\n",
       "      <td>0</td>\n",
       "      <td>Its hot Kinda wishing I did this Bummer Got ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>I hate hiccups  I've had them all day.</td>\n",
       "      <td>0</td>\n",
       "      <td>I hate hiccups I have had them all day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3774</th>\n",
       "      <td>Outta wk in 10 mins &amp;amp; Running home cus my ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Outta wk in mins amp Running home cus my Bros ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3697</th>\n",
       "      <td>@JasonNegron My team didn't *pouting* I was ho...</td>\n",
       "      <td>1</td>\n",
       "      <td>JasonNegron My team did not pouting I was hopi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>yeah.  several years ago.  miss him every day ...</td>\n",
       "      <td>0</td>\n",
       "      <td>yeah several years ago miss him every day but ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 twitts  sentiment  \\\n",
       "293               arghhh pissed off that essendon lost           0   \n",
       "2876  @peterfacinelli ..watching son playing soccer....          1   \n",
       "1894    why doesn't my pic show up when I tweetsearch?           0   \n",
       "3988  @TiaMowry  CW sux for dropping the show. Thank...          1   \n",
       "83    @marcusjroberts working on a Sunday sucks  don...          0   \n",
       "1203  It's hot.  Kinda wishing I did this. Bummer. G...          0   \n",
       "560              I hate hiccups  I've had them all day.          0   \n",
       "3774  Outta wk in 10 mins &amp; Running home cus my ...          1   \n",
       "3697  @JasonNegron My team didn't *pouting* I was ho...          1   \n",
       "860   yeah.  several years ago.  miss him every day ...          0   \n",
       "\n",
       "                                           cleaned_text  \n",
       "293                arghhh pissed off that essendon lost  \n",
       "2876  peterfacinelli watching son playing soccerin t...  \n",
       "1894     why does not my pic show up when I tweetsearch  \n",
       "3988  TiaMowry CW sux for dropping the show Thanks G...  \n",
       "83    marcusjroberts working on a Sunday sucks do no...  \n",
       "1203  Its hot Kinda wishing I did this Bummer Got ta...  \n",
       "560              I hate hiccups I have had them all day  \n",
       "3774  Outta wk in mins amp Running home cus my Bros ...  \n",
       "3697  JasonNegron My team did not pouting I was hopi...  \n",
       "860   yeah several years ago miss him every day but ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(data['cleaned_text'], data['sentiment'], test_size=0.20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = x_train.apply(lambda x: len(x)).max()\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 3000\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train_seq = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_seq = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "x_train_padded = pad_sequences(x_train_seq, maxlen = max_len)\n",
    "x_test_padded = pad_sequences(x_test_seq, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8516"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3769    With sister arianne httpplurkcompxkesi\n",
      "Name: cleaned_text, dtype: object\n",
      "[[625, 443, 2428, 2429]]\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0  625  443 2428 2429]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[:1])\n",
    "print(x_train_seq[:1])\n",
    "print(x_train_padded[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 µs, sys: 1e+03 ns, total: 12 µs\n",
      "Wall time: 22.6 µs\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "embedding_vectors = {}\n",
    "with open('/home/tim/trained/glove/glove.6B.50d.txt', 'r', encoding='utf-8') as file:\n",
    "    for row in file:\n",
    "        values = row.split(' ')\n",
    "        word = values[0]\n",
    "        weights = np.asarray([float(val) for val in values[1:]])\n",
    "        embedding_vectors[word] = weights\n",
    "\n",
    "print(len(embedding_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hahaha', 'lmao', 'bday', 'mileycyrus', 'tommcfly']\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 50\n",
    "if max_words is not None:\n",
    "    vocab_len = max_words\n",
    "else:\n",
    "    vocab_len = len(word_index)+1\n",
    "    \n",
    "embedding_matrix = np.zeros((vocab_len, emb_dim))\n",
    "oov_count = 0\n",
    "oov_words = []\n",
    "for word, idx in word_index.items():\n",
    "    if idx < vocab_len:\n",
    "        embedding_vector = embedding_vectors.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "        else:\n",
    "            oov_count +=1\n",
    "            oov_words.append(word)\n",
    "            \n",
    "print(oov_words[0:5])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375 out of 3000 words were OOV\n"
     ]
    }
   ],
   "source": [
    "print(oov_count, 'out of', vocab_len, 'words were OOV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 50)          150000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 8)                 1888      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 152,049\n",
      "Trainable params: 152,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_len, emb_dim, trainable=True, weights= [embedding_matrix]))\n",
    "model.add(LSTM(8))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation= 'sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics= ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100/100 [==============================] - 114s 1s/step - loss: 0.6901 - accuracy: 0.5203 - val_loss: 0.6804 - val_accuracy: 0.5675\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 86s 855ms/step - loss: 0.6547 - accuracy: 0.6291 - val_loss: 0.6346 - val_accuracy: 0.6425\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 86s 856ms/step - loss: 0.5917 - accuracy: 0.6938 - val_loss: 0.6014 - val_accuracy: 0.6425\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 88s 878ms/step - loss: 0.5211 - accuracy: 0.7516 - val_loss: 0.5827 - val_accuracy: 0.6938\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 86s 863ms/step - loss: 0.4563 - accuracy: 0.7956 - val_loss: 0.5873 - val_accuracy: 0.6925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efc651887f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_padded, np.asarray(y_train), epochs=5, validation_data=(x_test_padded, np.asarray(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tim/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /home/tim/trained/imdbglove/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('/home/tim/trained/imdbglove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205     charbrum deliver weekends nickbev place order ...\n",
      "2075                 mileycyrus hated lady gaga ow looove\n",
      "477     well six months trying get pregnant still go h...\n",
      "2446                          accelerate ps hang sometime\n",
      "2662                      iamthecrime awww change I liked\n",
      "1577    Kind dealbreaker u discover hot guy u know kin...\n",
      "3444    CYHSYtheband I shall dance I really want satan...\n",
      "3956    nlowenlsu watch next week I even remind make s...\n",
      "670     A random BEAUTIFUL baby gave hug shopping So p...\n",
      "3448    enjoyed Plymouth uni Summer Ball last nite awe...\n",
      "263                                 therealtommyg LIARRRR\n",
      "3698             chrisaffair Warped tour Our summer dates\n",
      "3803                               YoungCash Awwwww sweet\n",
      "3059    want change hair againn looove add hair ate tw...\n",
      "2161    Just got great feedback negotiation nice disco...\n",
      "1547                     Didnt good day feeling kinda sad\n",
      "640     pod doesnt enough space better start saving mo...\n",
      "1248                  missing sun Stuck windowless office\n",
      "1230                                    I troubled little\n",
      "168                       still waiting UPS guy damn want\n",
      "1551    according Cedar Point Rider Safety guide I lbs...\n",
      "1888                                              go work\n",
      "2014                               onepov bring home gold\n",
      "30             hummm goodnight twitter ending upset state\n",
      "2575                      littlefishey Thank I try change\n",
      "3322         JoelMadden hahahaha lolz best idea ive heard\n",
      "693                            face reallly fucking hurts\n",
      "1275              heidimontag dont know im UK yet jealous\n",
      "2100    goin aunt geris gon na play lax w brielle tomo...\n",
      "2271    mzmraz yep yep extremely tired already tour US...\n",
      "3873        dear payday please come faster im excited see\n",
      "3000                                Alright trying Laters\n",
      "362     How Lars Fredrickson Rancid sing like thathe a...\n",
      "3739    idreamofjeanny I mean I told drink drive I bel...\n",
      "2302                             party working hard today\n",
      "712                    VampireAyna Buhu I getting Ah crap\n",
      "3238    Dancing To Lady Gaga In My Undiess Summmeeerrr...\n",
      "1625    omg I love new car wish I could keep old one T...\n",
      "2002                                           lonely day\n",
      "357         janexdoe worry disappear easilly I gb thru gb\n",
      "184        megsi PatsyTravers voting working soooi behind\n",
      "979                                Disappointed breakfast\n",
      "649     Was vanessa today went memory ran errands Stop...\n",
      "2092                  httptwitpiccomgst Random Shot bored\n",
      "3245           mining long tail negative search terms PPC\n",
      "1546    The day I leave car across street day oil chan...\n",
      "1035    noooooooooo Your iPhone updated time activatio...\n",
      "3994    saravananr HeyCameron thejetset thanks kind wo...\n",
      "825     Going away Korea two months going miss bfmy fr...\n",
      "549     Monica Ooh ok Heard amazing unfortunately avai...\n",
      "2238      Going buy favorite isotonic drink pluswill back\n",
      "2277             I go swimming baby I done thousand years\n",
      "1814                                   xMarshmellows Awww\n",
      "3262    httptwitpiccomipyl Climbing bed watch Netflix ...\n",
      "1177    ChrisGorham Oh god show way stressful But I lo...\n",
      "3072                    much fun shopping spreelots stuff\n",
      "1595    I like sleeping home I much little girl At lea...\n",
      "1930    Got ta go work For Hours UGH And heres thing I...\n",
      "3850                     jamesiwilliams I luv Hello Kitty\n",
      "318     TOUCHOFCLASSENT yeah closed early like I start...\n",
      "Name: cleaned_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(x_test.head(60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    4\n",
      "  255    2  509 1149  651  192  339  370 1578]\n"
     ]
    }
   ],
   "source": [
    "test = (x_test_padded[205])\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'charbrum deliver weekends nickbev place order take tea'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[205]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.52215976],\n",
       "       [0.5244863 ],\n",
       "       [0.76222414],\n",
       "       [0.5832455 ],\n",
       "       [0.4917481 ],\n",
       "       [0.53436553],\n",
       "       [0.7007488 ],\n",
       "       [0.6112803 ],\n",
       "       [0.3243677 ],\n",
       "       [0.4850196 ],\n",
       "       [0.7112573 ]], dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre = model.predict((test))\n",
    "len(pre)\n",
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7282256]], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = ['the movie was good, i will recommend to watch']\n",
    "tokenizer.fit_on_texts(sample_text)\n",
    "seq = tokenizer.texts_to_sequences(sample_text)\n",
    "pad = pad_sequences(seq, maxlen=135)\n",
    "predictions = model.predict(pad)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
